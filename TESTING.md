# ğŸ§ª Akira Traders - DocumentaciÃ³n de Testing

## ğŸ“‹ Resumen Ejecutivo

El **mÃ³dulo de testing de Akira Traders** valida que el cÃ³digo implementado cumple con las especificaciones y documentaciÃ³n del proyecto. Incluye tests de cumplimiento, validaciÃ³n, integraciÃ³n y unitarios.

### ğŸ¯ Objetivos

1. **Validar cumplimiento** con ARCHITECTURE.md y methodology.md
2. **Verificar integridad** de datos y schemas
3. **Garantizar funcionalidad** de scripts y APIs
4. **Mantener calidad** del cÃ³digo

---

## ğŸ“Š Estructura del MÃ³dulo de Testing

```
tests/
â”œâ”€â”€ __init__.py                          # Paquete de tests
â”œâ”€â”€ README.md                            # DocumentaciÃ³n de tests
â”œâ”€â”€ conftest.py                          # ConfiguraciÃ³n global de pytest
â”‚
â”œâ”€â”€ compliance/                          # Tests de cumplimiento
â”‚   â””â”€â”€ test_documentation_compliance.py # Valida contra documentaciÃ³n
â”‚
â”œâ”€â”€ unit/                                # Tests unitarios
â”‚   â”œâ”€â”€ test_validation.py              # ValidaciÃ³n de datos/schemas
â”‚   â””â”€â”€ test_scripts.py                 # Tests de scripts Python
â”‚
â”œâ”€â”€ integration/                         # Tests de integraciÃ³n
â”‚   â””â”€â”€ test_backend_api.py             # Tests de API REST
â”‚
â””â”€â”€ fixtures/                            # Datos de ejemplo
    â””â”€â”€ sample_evaluations.json         # Evaluaciones de muestra

pytest.ini                               # ConfiguraciÃ³n de pytest
run_tests.py                             # Script ejecutor de tests
TESTING.md                               # Esta documentaciÃ³n
```

---

## ğŸš€ EjecuciÃ³n RÃ¡pida

### Ejecutar todos los tests

```bash
python run_tests.py
```

### Ejecutar con cobertura

```bash
python run_tests.py --coverage
```

### Ejecutar tests especÃ­ficos

```bash
python run_tests.py --compliance  # Solo cumplimiento
python run_tests.py --validation  # Solo validaciÃ³n
python run_tests.py --integration # Solo integraciÃ³n
python run_tests.py --unit        # Solo unitarios
```

---

## ğŸ“š CategorÃ­as de Tests

### 1. **Compliance Tests** (Cumplimiento)

**UbicaciÃ³n**: `tests/compliance/test_documentation_compliance.py`

**Objetivo**: Validar que el cÃ³digo cumple con las especificaciones documentadas.

**Clases de Tests**:

#### TestRiskProfileCompliance
Valida perfiles de riesgo segÃºn `docs/methodology.md`:

- âœ… Conservative (ROI 10-30%, DD â‰¤10%, WR â‰¥60%)
- âœ… Moderate (ROI 20-60%, DD â‰¤20%, WR â‰¥55%)
- âœ… Aggressive (ROI 40-200%, DD â‰¤35%, WR â‰¥50%)

```python
def test_methodology_file_exists(self):
    """Verifica que methodology.md existe"""

def test_risk_profiles_documented(self):
    """Verifica que los 3 perfiles estÃ©n documentados"""

def test_risk_profile_metrics_documented(self):
    """Verifica que todas las mÃ©tricas clave estÃ©n documentadas"""
```

#### TestArchitectureCompliance
Valida estructura segÃºn `ARCHITECTURE.md`:

- âœ… Directorios requeridos (docs/, scripts/, backend/, frontend/)
- âœ… Scripts requeridos (validate.py, analyze_metrics.py, consolidate.py)
- âœ… DocumentaciÃ³n completa

```python
def test_required_directories_exist(self):
    """Verifica que todos los directorios requeridos existan"""

def test_required_scripts_exist(self):
    """Verifica que todos los scripts requeridos existan"""
```

#### TestAPIEndpointCompliance
Valida endpoints del backend:

- âœ… `/api/health` - Health check
- âœ… `/api/validate` - ValidaciÃ³n de evaluaciones
- âœ… `/api/analyze` - AnÃ¡lisis de mÃ©tricas
- âœ… `/api/evaluations` - CRUD de evaluaciones

```python
def test_backend_has_health_endpoint(self):
    """Verifica que existe el endpoint de health check"""

def test_backend_has_crud_endpoints(self):
    """Verifica que existan los endpoints CRUD"""
```

#### TestScoringSystemCompliance
Valida sistema de scoring:

- âœ… Pesos por perfil (Conservative, Moderate, Aggressive)
- âœ… Rangos de scores (0-39: Pobre, 85-100: Excelente)
- âœ… MÃ©tricas ponderadas

#### TestDocumentationQuality
Valida calidad de documentaciÃ³n:

- âœ… README con secciones requeridas
- âœ… Diagramas en ARCHITECTURE.md
- âœ… Tablas de contenido

**Ejecutar**:
```bash
pytest tests/compliance/ -v
# o
python run_tests.py --compliance
```

---

### 2. **Validation Tests** (ValidaciÃ³n)

**UbicaciÃ³n**: `tests/unit/test_validation.py`

**Objetivo**: Validar integridad y correctitud de datos.

**Clases de Tests**:

#### TestTraderEvaluationSchema
Valida estructura de evaluaciones:

```python
def test_valid_evaluation_structure(self, valid_evaluation):
    """Verifica que una evaluaciÃ³n vÃ¡lida tenga todos los campos"""

def test_risk_profile_enum(self, valid_evaluation):
    """Verifica que el perfil de riesgo sea vÃ¡lido"""

def test_metrics_ranges(self, valid_evaluation):
    """Verifica que las mÃ©tricas estÃ©n dentro de rangos vÃ¡lidos"""
```

**Validaciones**:
- âœ… Campos requeridos presentes
- âœ… Tipos de datos correctos
- âœ… Enums vÃ¡lidos (risk_profile, style, copy_mode)
- âœ… Rangos de mÃ©tricas (0-100% para DD y WR)
- âœ… Arrays con 2 elementos para rangos

#### TestRiskProfileValidation
Valida lÃ­mites por perfil:

```python
def test_moderate_profile_roi_range(self, moderate_profile_limits):
    """Verifica rango de ROI para perfil moderado"""

def test_conservative_is_more_restrictive_than_moderate(self):
    """Verifica que conservador sea mÃ¡s restrictivo"""
```

**Validaciones**:
- âœ… Conservative mÃ¡s restrictivo que Moderate
- âœ… Aggressive menos restrictivo que Moderate
- âœ… LÃ­mites coherentes entre perfiles

#### TestDataIntegrity
Valida integridad de datos existentes:

```python
def test_can_load_evaluation_files(self):
    """Verifica que archivos existentes sean JSON vÃ¡lidos"""

def test_timestamp_format_is_iso8601(self):
    """Verifica formato ISO 8601 en timestamps"""
```

**Ejecutar**:
```bash
pytest tests/unit/test_validation.py -v
# o
python run_tests.py --validation
```

---

### 3. **Backend API Tests** (IntegraciÃ³n)

**UbicaciÃ³n**: `tests/integration/test_backend_api.py`

**Objetivo**: Validar funcionalidad de endpoints REST.

**Clases de Tests**:

#### TestHealthEndpoint
```python
def test_health_endpoint_exists(self, client):
    """Verifica que /api/health existe y responde"""

def test_health_returns_json(self, client):
    """Verifica que retorna JSON vÃ¡lido"""
```

#### TestValidateEndpoint
```python
def test_validate_accepts_json(self, client, sample_evaluation):
    """Verifica que acepta evaluaciones JSON"""

def test_validate_response_structure(self, client):
    """Verifica estructura de respuesta"""
```

#### TestEvaluationsCRUD
```python
def test_get_evaluations_returns_array(self, client):
    """Verifica que GET retorna array de evaluaciones"""

def test_post_evaluation_endpoint_exists(self, client):
    """Verifica que POST funciona"""
```

**Ejecutar**:
```bash
pytest tests/integration/ -v
# o
python run_tests.py --integration
```

**Nota**: Requiere que el backend estÃ© disponible.

---

### 4. **Scripts Tests** (Unitarios)

**UbicaciÃ³n**: `tests/unit/test_scripts.py`

**Objetivo**: Validar scripts Python del proyecto.

**Clases de Tests**:

#### TestScriptsExistence
```python
@pytest.mark.parametrize("script_name", ["validate.py", "analyze_metrics.py", "consolidate.py"])
def test_script_exists(self, script_name):
    """Verifica que el script existe"""
```

#### TestValidateScript
```python
def test_validate_script_can_run(self):
    """Verifica que validate.py se pueda ejecutar"""
```

#### TestUtilsModule
```python
def test_utils_is_python_package(self):
    """Verifica que utils/ sea un paquete Python"""
```

**Ejecutar**:
```bash
pytest tests/unit/test_scripts.py -v
# o
python run_tests.py --unit
```

---

## ğŸ› ï¸ ConfiguraciÃ³n

### pytest.ini

ConfiguraciÃ³n principal de pytest:

```ini
[pytest]
testpaths = tests
python_files = test_*.py
markers =
    compliance: Tests de cumplimiento
    validation: Tests de validaciÃ³n
    integration: Tests de integraciÃ³n
    unit: Tests unitarios
```

### conftest.py

Fixtures globales disponibles:

```python
@pytest.fixture(scope="session")
def base_dir():
    """Directorio base del proyecto"""

@pytest.fixture(scope="session")
def scripts_dir():
    """Directorio de scripts"""
```

---

## ğŸ“Š Cobertura de CÃ³digo

### Generar reporte de cobertura

```bash
python run_tests.py --coverage
```

### Ver reporte HTML

```bash
open htmlcov/index.html  # macOS
xdg-open htmlcov/index.html  # Linux
```

### Objetivos de cobertura

- **Compliance**: 100% (toda la documentaciÃ³n validada)
- **Scripts**: â‰¥80% (funciones crÃ­ticas)
- **Backend**: â‰¥70% (endpoints principales)
- **Global**: â‰¥75%

---

## ğŸ” Uso Avanzado

### Ejecutar test especÃ­fico

```bash
pytest tests/compliance/test_documentation_compliance.py::TestRiskProfileCompliance::test_methodology_file_exists -v
```

### Ejecutar con markers

```bash
pytest -m compliance -v
pytest -m "unit and not slow" -v
```

### Modo verbose

```bash
pytest -vv -s  # Extra verbose con prints
```

### Generar reporte JUnit XML

```bash
python run_tests.py --report
```

El reporte se guarda en `test_reports/junit_report_<timestamp>.xml`

---

## ğŸ“ˆ IntegraciÃ³n Continua (CI/CD)

### GitHub Actions (ejemplo)

```yaml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run tests
        run: python run_tests.py --coverage --report
      - name: Upload coverage
        uses: codecov/codecov-action@v3
```

---

## ğŸ› Troubleshooting

### Error: ModuleNotFoundError

```bash
export PYTHONPATH="${PYTHONPATH}:$(pwd):$(pwd)/scripts:$(pwd)/backend"
```

### Error: No module named 'pytest'

```bash
pip install pytest pytest-cov
```

### Tests muy lentos

```bash
pytest -m "not slow" -v
```

### Ver solo failures

```bash
pytest --tb=short -x  # Stop en primer failure
pytest --lf  # Solo re-ejecutar failures anteriores
```

---

## ğŸ“ Convenciones de Testing

### Nomenclatura

- **Archivos**: `test_*.py`
- **Clases**: `Test*`
- **MÃ©todos**: `test_*`

### Estructura de un test

```python
class TestFeature:
    """DescripciÃ³n de quÃ© se testea"""

    @pytest.fixture
    def setup_data(self):
        """Fixture local"""
        return {"key": "value"}

    def test_something(self, setup_data):
        """DescripciÃ³n clara del test"""
        # Arrange
        expected = "value"

        # Act
        result = setup_data["key"]

        # Assert
        assert result == expected
```

### Asserts

```python
# Preferir asserts con mensaje
assert condition, "Mensaje de error descriptivo"

# Comparaciones
assert actual == expected
assert value in collection
assert instance isinstance(obj, Class)
```

---

## ğŸ“š Recursos

- [pytest Documentation](https://docs.pytest.org/)
- [pytest-cov](https://pytest-cov.readthedocs.io/)
- [Testing Best Practices](https://docs.python-guide.org/writing/tests/)

---

## âœ… Checklist para Pull Requests

Antes de crear un PR, verificar:

- [ ] Todos los tests pasan
- [ ] Cobertura â‰¥ objetivo (75%)
- [ ] Nuevas features tienen tests
- [ ] Tests de compliance actualizados si hay cambios en docs
- [ ] Sin warnings de pytest
- [ ] Reporte de cobertura revisado

---

## ğŸ¯ Roadmap de Testing

### Fase Actual âœ…
- [x] Tests de cumplimiento con documentaciÃ³n
- [x] Tests de validaciÃ³n de datos
- [x] Tests de integraciÃ³n de API
- [x] Tests unitarios de scripts

### PrÃ³ximos Pasos ğŸ“…
- [ ] Tests end-to-end del flujo completo
- [ ] Tests de performance
- [ ] Tests de seguridad (validaciÃ³n de inputs)
- [ ] Mutation testing
- [ ] Property-based testing (Hypothesis)

---

**Ãšltima actualizaciÃ³n**: 2025-01-09
**VersiÃ³n**: 1.0.0
**Autor**: Akira Traders Team
